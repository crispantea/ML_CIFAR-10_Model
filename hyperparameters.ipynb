{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameters and Their Impact on Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Hyperparameter**       | **Overfitting**                                   | **Accuracy**                                      | **Cost**                                              |\n",
    "|--------------------------|--------------------------------------------------|---------------------------------------------------|-------------------------------------------------------|\n",
    "| **Filter/Kernel Size**   | Larger filters may overfit to broader patterns; smaller filters capture localized features better, aiding generalization. | Optimal size enhances feature extraction; mismatch with data scale can hurt accuracy. | Larger filters require more computation and memory.   |\n",
    "| **Number of Filters**    | More filters capture detail but can overfit to noise; fewer filters lower overfitting risk. | Enough filters capture diverse features; too few may miss patterns. | More filters increase memory and computational demand. |\n",
    "| **Stride**               | Larger strides may underfit by skipping details; smaller strides capture finer information. | Proper stride helps balance detail and efficiency; mismatched stride may harm accuracy. | Larger strides reduce computation time but lower detail. |\n",
    "| **Padding**              | Minimal impact on overfitting, but improper padding can truncate features and affect learning. | Helps preserve feature map size; poor padding can hinder feature extraction. | Minimal cost, but essential for preserving spatial dimensions. |\n",
    "| **Pooling Size & Type**  | Max pooling reduces overfitting by focusing on dominant features; average pooling may retain noise. | Proper pooling extracts essential features, enhancing accuracy; poor choice harms recognition. | Larger pooling sizes lower computational requirements. |\n",
    "| **Dropout Rate**         | High dropout reduces overfitting by preventing reliance on specific neurons. | Proper dropout improves generalization by preventing over-reliance on individual features. | No additional compute for inference; minimal during training. |\n",
    "| **Weight Initialization**| Poor initialization can cause overfitting to early patterns, while good initialization stabilizes learning. | Effective initialization speeds up convergence and boosts accuracy. | Minimal computational cost but significantly affects training speed. |\n",
    "| **Activation Function**  | Certain functions, like ReLU, help avoid overfitting by reducing vanishing gradient issues. | Correct activation improves learning complex patterns; poor choice limits learning capability. | Minimal impact on cost but influences efficiency and performance. |\n",
    "| **Batch Size**           | Large batches risk memorization, increasing overfitting; smaller batches can reduce it with varied updates. | Proper batch size stabilizes convergence; too small or large can hurt accuracy. | Larger batches require more memory but can speed up each epoch. |\n",
    "| **Learning Rate**        | High rates risk missing finer patterns, potentially overfitting; lower rates reduce this risk. | Properly tuned rates improve accuracy; extremes in either direction can degrade learning. | Impacts training time; high rates save time but may destabilize. |\n",
    "| **Number of Epochs**     | More epochs increase overfitting risk; early stopping helps prevent this. | Sufficient epochs allow deeper learning and improve accuracy if balanced. | Increased epochs lead to longer training time.          |\n",
    "| **Optimizer**            | Adaptive optimizers like Adam may overfit more than SGD. | Adaptive optimizers improve accuracy generally; poor choice limits learning. | Adaptive optimizers may use more resources than SGD. |\n",
    "| **Regularization (L2)**  | Regularization reduces overfitting by penalizing large weights, preventing complex models. | Enhances model generalization and stability, reducing risk of overfitting. | Adds minimal computation to each update step.          |\n",
    "\n",
    "This table reflects the typical sequence of setting or tuning these hyperparameters within a CNN, from defining the **convolutional layer specifics** (filters, kernel size, stride, and padding) to **regularization and optimization strategies**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **More Detailed explanation about each Hyperparameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Filter/Kernel Size**\n",
    "\n",
    "**Description:** The filter (or kernel) size in a convolutional neural network (CNN) defines the dimensions of the filter applied to the input data to extract features.\n",
    "\n",
    "- **What It Does**: Filters detect patterns in the data by \"scanning\" over it. The size of the filter impacts what types of features are captured:\n",
    "  - **Small Filters** (e.g., (3 x 3): Capture fine, local details like edges or textures, commonly used in early layers to detect basic features.\n",
    "  - **Large Filters** (e.g., (5 x 5) or larger): Capture broader, more global patterns and relationships in the data; often applied in deeper layers.\n",
    "\n",
    "Choosing the filter size helps balance the model’s ability to detect both small and large patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Number of Filters (Depth)**\n",
    "\n",
    "**Description:** Number of filters used in a convolutional layer.\n",
    "\n",
    "- **More filters** capture a wider variety of features, allowing the model to learn more complex patterns.\n",
    "- **Drawbacks**: Increasing filters raises computational cost and can lead to overfitting, especially with limited data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stride**\n",
    "\n",
    "**Description:** The number of pixels the filter moves across the input during convolution.\n",
    "\n",
    "- **Larger strides** downsample the spatial dimensions of the feature map, reducing computational load.\n",
    "- **Drawbacks**: Using larger strides may result in the loss of fine-grained details in the feature extraction process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Padding**\n",
    "\n",
    "**Description:** Adds pixels around the input to maintain spatial dimensions during convolution.\n",
    "\n",
    "- **Purpose**: Helps maintain the input size in deep networks, preventing rapid reduction in the feature map size.\n",
    "- **Types**: Common types include 'valid' (no padding) and 'same' (padding to keep the size unchanged).\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pooling Size and Type**\n",
    "\n",
    "**Description:** The pooling layer reduces dimensionality by downsampling the feature map.\n",
    "\n",
    "- **Purpose**: Reduces computation by decreasing the size of the feature map, adding translation invariance.\n",
    "- **Common Types**:\n",
    "  - **Max Pooling**: Captures the dominant features by selecting the maximum value from a region.\n",
    "  - **Average Pooling**: Computes the average value, which can smooth out features but may lose important details.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropout Rate**\n",
    "\n",
    "**Description:** Fraction of neurons randomly dropped during training to prevent overfitting.\n",
    "\n",
    "- **Purpose**: Dropout improves generalization by making the network less sensitive to specific neurons, encouraging the model to learn more robust features.\n",
    "- **Typical Values**: Common dropout rates range from 0.2 to 0.5, depending on the model architecture and dataset.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Weight Initialization Method**\n",
    "\n",
    "**Description:** Defines how initial weights are set in a neural network (e.g., Xavier, He initialization).\n",
    "\n",
    "- **Purpose**: Proper weight initialization ensures faster convergence during training and improves overall model performance.\n",
    "- **Common Methods**:\n",
    "  - **Xavier Initialization**: Suitable for sigmoid and tanh activations; keeps the scale of the gradients similar across layers.\n",
    "  - **He Initialization**: Recommended for ReLU activations; helps in maintaining a good variance in the weights for deeper networks.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activation Function**\n",
    "\n",
    "**Description:** A non-linear function applied to each layer’s output to introduce non-linearity into the model.\n",
    "\n",
    "- **Purpose**: The choice of activation function significantly affects convergence speed and overall model performance.\n",
    "- **Common Functions**:\n",
    "  - **ReLU (Rectified Linear Unit)**: Widely used due to its simplicity and effectiveness; helps avoid the vanishing gradient problem.\n",
    "  - **Sigmoid**: Often used in binary classification, but can lead to vanishing gradients.\n",
    "  - **Tanh**: Similar to sigmoid but outputs values between -1 and 1; generally performs better than sigmoid.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Batch Size**\n",
    "\n",
    "**Description**: The number of samples processed before updating the model's weights.\n",
    "\n",
    "- **Larger Batch Sizes**: A large batch size (e.g., 128 or 256) tends to produce more stable and smoother updates in the weights, as each update is based on more samples, reducing gradient variation.\n",
    "- **Smaller Batch Sizes**: A small batch size (e.g., 16 or 32) involves more updates per epoch, with each update based on fewer data points. This can lead to \"noisier\" updates in the weights, which may help the model escape local minima and improve generalization.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Learning Rate**\n",
    "\n",
    "**Description**: Determines the step size for updating weights during training.\n",
    "\n",
    "- **High Learning Rates**: Can speed up training but risk overshooting the optimal weights, potentially leading to divergence.\n",
    "- **Low Learning Rates**: May improve convergence and lead to more precise weight adjustments, but can significantly increase training time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Number of Epochs**\n",
    "\n",
    "**Description:** The number of times the model iterates over the entire dataset.\n",
    "\n",
    "- A sufficient number of epochs is necessary for effective learning, but **excessive epochs can lead to overfitting**, where the model performs well on training data but poorly on unseen data.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizer Choice**\n",
    "\n",
    "**Description:** The algorithm used to update weights during training (e.g., SGD, Adam).\n",
    "\n",
    "- **Adam**: Adapts learning rates for each parameter, often leading to faster convergence and better performance on various tasks.\n",
    "- **SGD (Stochastic Gradient Descent)**: Provides smooth convergence but may require more tuning of the learning rate and can be sensitive to the initial conditions.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regularization Strength (e.g., L2 Regularization)**\n",
    "\n",
    "**Description:** A penalty added to the loss function to discourage large weights.\n",
    "\n",
    "- **Purpose**: Helps control overfitting by discouraging the model from becoming overly complex and encourages the learning of simpler models that generalize better to unseen data.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note: Relationship Between Learning Rate and Batch Size**\n",
    "\n",
    "With **smaller batch sizes**, it’s ideal to **use a lower learning rate** to avoid sharp oscillations, as updates are more variable due to fewer samples. Conversely, **larger batch sizes** allow for a **higher learning rate** without sacrificing stability, as gradient calculations are more accurate with more samples, enabling stronger weight updates.\n",
    "\n",
    "#### **Trade-Offs:**\n",
    "- **Small batch size with high learning rate:** Can lead to unstable and erratic convergence.\n",
    "- **Large batch size with very low learning rate:** May cause very slow convergence.\n",
    "\n",
    "**Scaling Learning Rate with Batch Size**\n",
    "\n",
    "A common approach is to **scale the learning rate proportionally with batch size increases**, often by multiplying it by the square root of the batch size increase. This helps the model leverage stability from larger batches without excessively lowering the learning rate.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
